# ğŸ›ï¸ Retail Customer Insights Data Pipeline (PySpark + PostgreSQL)

This project builds an end-to-end **batch ETL pipeline** to generate customer-level insights for a retail business.  
The pipeline uses **PySpark** for data processing and **PostgreSQL** as the storage layer for the final curated data.

It processes multi-source data (CSV + JSON), performs transformations, enrichments, aggregations, and loads output into a PostgreSQL table.

---

## ğŸ§  Overview

The pipeline processes three raw datasets:

- `orders.csv` (order details)
- `customers.csv` (customer demographics)
- `products.json` (product master)

It generates customer-level insights such as:

- Total amount spent  
- Total number of orders  
- Average product rating  
- Customer city insights  

The final output is stored in PostgreSQL for analytics.

---

## ğŸ—ï¸ Architecture
Raw Data (CSV + JSON)
â†“
extract.py â†’ Load raw data using PySpark
transform.py â†’ Clean, transform & aggregate customer insights
load.py â†’ Load final output into PostgreSQL
main.py â†’ Run full pipeline end-to-end

---

## ğŸ§° Tech Stack

- **PySpark**
- **Python**
- **PostgreSQL**
- **SQLAlchemy + psycopg2**
- **CSV + JSON data sources**

---

## ğŸ“‚ Project Structure

retail_customer_insights/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ orders.csv
â”‚ â”œâ”€â”€ customers.csv
â”‚ â””â”€â”€ products.json
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ extract.py # Reads raw input data using PySpark
â”‚ â”œâ”€â”€ transform.py # Clean + join + aggregate to produce insights
â”‚ â”œâ”€â”€ load.py # Loads final curated dataframe into PostgreSQL
â”‚ â””â”€â”€ main.py # Orchestrator: extract â†’ transform â†’ load
â”‚
â”œâ”€â”€ config/
â”‚ â””â”€â”€ db_config.py # PostgreSQL connection info
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ§° Setup Instructions

```bash
# 1ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 2ï¸âƒ£ Run only extraction
python scripts/extract.py

# 3ï¸âƒ£ Run transformation
python scripts/transform.py

# 4ï¸âƒ£ Load curated data into PostgreSQL
python scripts/load.py

# 5ï¸âƒ£ (Optional) Run complete end-to-end pipeline
python scripts/main.py

## ğŸ—ƒï¸ PostgreSQL Output Table

Table Name: customer_insights

Column	Description
customer_id	Unique customer identifier
name	Customer name
customer_city	City extracted from customers.csv
total_spent	Total order value
total_orders	Number of orders placed
avg_rating	Average rating across all purchases

## âš™ï¸ PySpark Transformations Performed

Load CSV + JSON input data
Handle nulls & cast datatypes

Join:
orders â†” customers
orders â†” products

Add derived fields:
total_amount = quantity Ã— price

Compute customer metrics:
total_spent
total_orders
avg_rating

Return final curated customer_insights dataframe

## ğŸ’¾ Example Output
+-----------+--------+-------------+-----------+------------+----------+
|customer_id| name   |customer_city|total_spent|total_orders|avg_rating|
+-----------+--------+-------------+-----------+------------+----------+
|        101| Neha   | Mumbai      |     2000  |     2      |   5.0    |
|        102| Ashok  | Delhi       |     1800  |     2      |   4.0    |
|        103| Raj    | Bangalore   |     1200  |     1      |   3.0    |
+-----------+--------+-------------+-----------+------------+----------+

